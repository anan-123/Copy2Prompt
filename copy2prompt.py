# -*- coding: utf-8 -*-
"""final_dsc291.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zw1zxg7MSFMvzzTsfub5jC-u0wA0WI9i
"""

!pip install opencv-python

!pip install git+https://github.com/openai/CLIP.git

!pip install compel

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from skimage.metrics import structural_similarity as ssim
from skimage.feature import hog
import cv2

class Copy2PromptPipeline:
    def __init__(self, analyzer, generator):
        self.analyzer = analyzer
        self.generator = generator

    def process_image(
        self,
        image: Image.Image,
        custom_prompt: str = None,
        num_variations: int = 1,
        seed: int = None,
        num_inference_steps=40,
        guidance_scale=7.5,
        max_tokens=512,
    ) -> dict:
        print("Step 1: Analyzing image with LLaVA...")
        detailed_prompt = self.analyzer.analyze_image(
            image, custom_prompt, max_tokens=max_tokens
        )
        print(f"Generated prompt: {detailed_prompt}")

        print(f"Step 2: Generating {num_variations} image variation(s) with SDXL...")
        generated_images = []

        for i in range(num_variations):
            variation_seed = seed + i if seed is not None else None
            generated_image = self.generator.generate_image(
                prompt=detailed_prompt,
                seed=variation_seed,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
            )
            generated_images.append(generated_image)
            print(f"Generated variation {i+1}/{num_variations}")

        return {
            "original_image": image,
            "generated_prompt": detailed_prompt,
            "generated_images": generated_images,
        }

    def calculate_image_similarity(self, img1: Image.Image, img2: Image.Image) -> dict:
        """Calculate selected similarity metrics between two images"""

        # Convert PIL images to numpy arrays
        img1_array = np.array(img1.convert('RGB'))
        img2_array = np.array(img2.convert('RGB'))

        # Resize images to same size for comparison
        target_size = (256, 256)
        img1_resized = cv2.resize(img1_array, target_size)
        img2_resized = cv2.resize(img2_array, target_size)

        # Convert to grayscale for some metrics
        img1_gray = cv2.cvtColor(img1_resized, cv2.COLOR_RGB2GRAY)
        img2_gray = cv2.cvtColor(img2_resized, cv2.COLOR_RGB2GRAY)

        scores = {}

        # 1. Structural Similarity Index (SSIM)
        ssim_score = ssim(img1_gray, img2_gray, data_range=255)
        scores['ssim'] = ssim_score

        # 2. Multi-Scale SSIM (MS-SSIM)
        ms_ssim = self._calculate_ms_ssim(img1_gray, img2_gray)
        scores['ms_ssim'] = ms_ssim

        # 3. Mean Squared Error (lower is better)
        mse = np.mean((img1_resized.astype(float) - img2_resized.astype(float)) ** 2)
        scores['mse'] = mse

        # 4. LAB color space similarity
        img1_lab = cv2.cvtColor(img1_resized, cv2.COLOR_RGB2LAB)
        img2_lab = cv2.cvtColor(img2_resized, cv2.COLOR_RGB2LAB)
        lab_diff = np.mean(np.sqrt(np.sum((img1_lab.astype(float) - img2_lab.astype(float))**2, axis=2)))
        lab_similarity = max(0, 1 - (lab_diff / 100))  # Normalize to 0-1
        scores['lab_similarity'] = lab_similarity

        # 5. HOG feature similarity
        try:
            hog1 = hog(img1_gray, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)
            hog2 = hog(img2_gray, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)
            hog_similarity = cosine_similarity([hog1], [hog2])[0][0]
            scores['hog_similarity'] = hog_similarity
        except:
            scores['hog_similarity'] = 0.0

        # 6. Histogram correlation (bonus metric)
        hist1 = cv2.calcHist([img1_resized], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
        hist2 = cv2.calcHist([img2_resized], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
        hist_correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        scores['hist_correlation'] = hist_correlation

        # Overall similarity score (weighted combination)
        overall_score = (
            0.25 * ssim_score +
            0.20 * ms_ssim +
            0.20 * lab_similarity +
            0.15 * hog_similarity +
            0.10 * hist_correlation +
            0.10 * (1 - min(1.0, mse / 10000))  # Normalize MSE (lower is better)
        )
        scores['overall_similarity'] = overall_score

        return scores

    def _calculate_ms_ssim(self, img1, img2, weights=[0.0448, 0.2856, 0.3001, 0.2363, 0.1333]):
        """Multi-Scale SSIM calculation"""
        levels = len(weights)
        mssim = []

        for i in range(levels):
            ssim_val = ssim(img1, img2, data_range=255)
            mssim.append(ssim_val)

            if i < levels - 1:
                img1 = cv2.resize(img1, (img1.shape[1]//2, img1.shape[0]//2))
                img2 = cv2.resize(img2, (img2.shape[1]//2, img2.shape[0]//2))

        return np.prod([mssim[i]**weights[i] for i in range(levels)])

    def display_results(self, results: dict):
        original_image = results["original_image"]
        generated_images = results["generated_images"]
        num_images = len(generated_images)

        # Calculate similarity scores
        similarity_scores = []
        for i, gen_img in enumerate(generated_images):
            scores = self.calculate_image_similarity(original_image, gen_img)
            similarity_scores.append(scores)

        # Create subplot layout
        cols = min(3, num_images + 1)
        rows = (num_images + 1 + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if rows == 1:
            axes = axes.reshape(1, -1)

        # Display original image
        axes[0, 0].imshow(original_image)
        axes[0, 0].set_title("Original Image", fontsize=12, fontweight='bold')
        axes[0, 0].axis("off")

        # Display generated images with similarity scores
        for i, (img, scores) in enumerate(zip(generated_images, similarity_scores)):
            row = (i + 1) // cols
            col = (i + 1) % cols

            axes[row, col].imshow(img)

            # Create title with similarity score
            title = f"Variation {i+1}\nSimilarity: {scores['overall_similarity']:.3f}"
            axes[row, col].set_title(title, fontsize=10)
            axes[row, col].axis("off")

            # Add detailed scores as text below image
            score_text = f"SSIM: {scores['ssim']:.3f}\nLAB: {scores['lab_similarity']:.3f}"
            axes[row, col].text(0.5, -0.1, score_text,
                              transform=axes[row, col].transAxes,
                              ha='center', va='top', fontsize=8)

        # Hide unused subplots
        for i in range(num_images + 1, rows * cols):
            row = i // cols
            col = i % cols
            axes[row, col].axis("off")

        plt.tight_layout()
        plt.show()

        # Display detailed similarity analysis
        print("\n" + "="*70)
        print("IMAGE SIMILARITY ANALYSIS")
        print("="*70)

        # Rank variations by overall similarity
        ranked_variations = sorted(enumerate(similarity_scores),
                                 key=lambda x: x[1]['overall_similarity'],
                                 reverse=True)

        print(f"\nRANKING (Best to Worst Similarity to Original):")
        print("-" * 50)
        for rank, (var_idx, scores) in enumerate(ranked_variations, 1):
            print(f"{rank}. Variation {var_idx + 1}: {scores['overall_similarity']:.4f}")

        print(f"\nDETAILED SIMILARITY METRICS:")
        print("-" * 50)
        for i, scores in enumerate(similarity_scores):
            print(f"\nVariation {i+1}:")
            print(f"  â€¢ Overall Similarity:    {scores['overall_similarity']:.4f}")
            print(f"  â€¢ SSIM (Structure):      {scores['ssim']:.4f}")
            print(f"  â€¢ MS-SSIM (Multi-Scale): {scores['ms_ssim']:.4f}")
            print(f"  â€¢ LAB Color Similarity:  {scores['lab_similarity']:.4f}")
            print(f"  â€¢ HOG Feature Similarity:{scores['hog_similarity']:.4f}")
            print(f"  â€¢ Histogram Correlation: {scores['hist_correlation']:.4f}")
            print(f"  â€¢ MSE (Error):           {scores['mse']:.2f}")

        # Provide interpretation
        print(f"\nINTERPRETATION GUIDE:")
        print("-" * 50)
        print("â€¢ Overall Similarity: 0.0 (no similarity) â†’ 1.0 (identical)")
        print("â€¢ SSIM: Structural similarity (layout/composition)")
        print("â€¢ MS-SSIM: Multi-scale structural similarity")
        print("â€¢ LAB Similarity: Perceptually uniform color matching")
        print("â€¢ HOG Features: Shape and edge pattern similarity")
        print("â€¢ Histogram: Color distribution similarity")
        print("â€¢ MSE: Mean Squared Error (lower values = better match)")

        print("\nGenerated Prompt:")
        print("=" * 50)
        print(results["generated_prompt"])

        return similarity_scores

!unzip Archive.zip

!ls

import torch
from torchvision import transforms
from PIL import Image
import clip
import random
import numpy as np

class PromptOptimizerAgent:
    def __init__(self, generator, reference_image, base_prompt: str, device="cuda" if torch.cuda.is_available() else "cpu"):
        self.generator = generator
        self.reference_image = reference_image.convert('RGB')
        self.base_prompt = base_prompt
        self.device = device
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.quality_enhancers = [
            "highly detailed", "ultra realistic", "8k resolution", "masterpiece",
            "professional photography", "award winning", "trending on artstation",
            "hyperrealistic", "photorealistic", "studio quality", "crisp and clear",
            "sharp focus", "intricate details", "fine art quality"
        ]
        self.technical_specs = [
            "shot on Canon EOS R5", "85mm lens", "f/1.4 aperture", "shallow depth of field",
            "bokeh background", "long exposure", "HDR photography", "RAW format",
            "professional color grading", "film grain texture", "35mm film aesthetic",
            "medium format camera", "large format photography"
        ]
        self.composition_rules = [
            "rule of thirds", "leading lines", "symmetrical composition", "golden ratio",
            "diagonal composition", "frame within frame", "negative space",
            "central composition", "dynamic composition", "balanced composition",
            "asymmetrical balance", "radial composition"
        ]

    def mutate_prompt(self, prompt):
        """Add one enhancement from each category to the prompt"""
        quality = random.choice(self.quality_enhancers)
        technical = random.choice(self.technical_specs)
        composition = random.choice(self.composition_rules)
        return f"{prompt}, {quality}, {technical}, {composition}"

    def evaluate(self, image):
        """Use CLIP to compute similarity to the reference image"""
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        ref_input = self.preprocess(self.reference_image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            ref_features = self.model.encode_image(ref_input)

        image_features /= image_features.norm(dim=-1, keepdim=True)
        ref_features /= ref_features.norm(dim=-1, keepdim=True)

        similarity = (image_features @ ref_features.T).item()
        return similarity

    def run(self, iterations=5):
        current_prompt = self.base_prompt
        best_prompt = current_prompt
        best_image = None
        best_score = -float('inf')

        for i in range(iterations):
            print(f"\n[Iteration {i+1}]")
            new_prompt = self.mutate_prompt(current_prompt)
            print("ðŸ“ Prompt:", new_prompt)

            # Generate image
            image = self.generator.generate_image(
                prompt=new_prompt,
                seed=random.randint(0, 9999),
                num_inference_steps=40,
                guidance_scale=7.5
            )

            # Evaluate with CLIP
            score = self.evaluate(image)
            print(f" CLIP Similarity Score: {score:.4f}")

            if score > best_score:
                best_score = score
                best_prompt = new_prompt
                best_image = image

        return best_prompt, best_image, best_score

import torch
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from diffusers import StableDiffusionXLPipeline
from compel import Compel, ReturnedEmbeddingsType
from PIL import Image
import matplotlib.pyplot as plt
import requests
from io import BytesIO
import os
from typing import List, Optional
import warnings
from pathlib import Path

class SDXLGenerator:
    def __init__(self, model_name="stabilityai/stable-diffusion-xl-base-1.0"):
        print(f"Loading SDXL model: {model_name}")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.pipeline = StableDiffusionXLPipeline.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            use_safetensors=True,
            variant="fp16" if torch.cuda.is_available() else None,
        )
        self.pipeline = self.pipeline.to(self.device)

        if torch.cuda.is_available():
            self.pipeline.enable_model_cpu_offload()
            self.pipeline.enable_vae_slicing()

        print(f"SDXL loaded on device: {self.device}")

        # Initialize Compel for handling long prompts
        self.compel = Compel(
            tokenizer=[self.pipeline.tokenizer, self.pipeline.tokenizer_2],
            text_encoder=[self.pipeline.text_encoder, self.pipeline.text_encoder_2],
            returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,
            requires_pooled=[False, True],
        )
        print("âœ“ Compel initialized for long prompt support")

    def generate_image(
        self,
        prompt: str,
        negative_prompt: str = None,
        num_inference_steps: int = 40,
        guidance_scale: float = 9,
        width: int = 1024,
        height: int = 1024,
        seed: int = None,
    ) -> Image.Image:

        if negative_prompt is None:
            negative_prompt = (
                "blurry, low quality, distorted, deformed, bad anatomy, bad proportions"
            )

        generator = torch.Generator(device=self.device)
        if seed is not None:
            generator.manual_seed(seed)

        # Process prompts with Compel to handle long prompts
        conditioning, pooled = self.compel(prompt)
        negative_conditioning, negative_pooled = self.compel(negative_prompt)

        # Generate with embeddings instead of text
        with torch.no_grad():
            result = self.pipeline(
                prompt_embeds=conditioning,
                pooled_prompt_embeds=pooled,
                negative_prompt_embeds=negative_conditioning,
                negative_pooled_prompt_embeds=negative_pooled,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                width=width,
                height=height,
                generator=generator,
            )

        return result.images[0]

import requests
from io import BytesIO

def load_image(path_or_url: str) -> Image.Image:
    if path_or_url.startswith("http"):
        response = requests.get(path_or_url)
        image = Image.open(BytesIO(response.content))
    else:
        image = Image.open(path_or_url)

    return image.convert("RGB")

sample_image = load_image("car.jpeg")
generator = SDXLGenerator()

agent = PromptOptimizerAgent(generator, sample_image, base_prompt="A supercar")
best_prompt, best_image, best_score = agent.run(iterations=5)

print("\n Best Prompt:", best_prompt)
print("Best CLIP Score:", best_score)
best_image.show()



sample_image = load_image("cat.jpeg")
agent = PromptOptimizerAgent(generator, sample_image, base_prompt="A cat")
best_prompt, best_image, best_score = agent.run(iterations=5)

print("\n Best Prompt:", best_prompt)
print("Best CLIP Score:", best_score)
best_image.show()

best_image

sample_image = load_image("book.jpeg")
agent = PromptOptimizerAgent(generator, sample_image, base_prompt="Book")
best_prompt, best_image, best_score = agent.run(iterations=5)

print("\n Best Prompt:", best_prompt)
print("Best CLIP Score:", best_score)
best_image

sample_image = load_image("lake.jpeg")
agent = PromptOptimizerAgent(generator, sample_image, base_prompt="Lake")
best_prompt, best_image, best_score = agent.run(iterations=5)

print("\n Best Prompt:", best_prompt)
print("Best CLIP Score:", best_score)
best_image

sample_image = load_image("people.jpeg")
agent = PromptOptimizerAgent(generator, sample_image, base_prompt="Sunset at the beach with a group of people")
best_prompt, best_image, best_score = agent.run(iterations=5)

print("\n Best Prompt:", best_prompt)
print("Best CLIP Score:", best_score)
best_image

